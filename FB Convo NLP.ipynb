{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rogge\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import json\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open() as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "dataframe = pd.DataFrame(data[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary column, null data, and any comments that are only weblinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe.columns = ['audio_files', 'content', 'files', 'gifs', 'photos', 'plan',\n",
    "       'reactions', 'sender_name', 'share', 'sticker', 'epoch time', 'type',\n",
    "       'videos']\n",
    "\n",
    "dataframe = dataframe.drop(labels = ['audio_files', 'files', 'gifs', 'plan', 'reactions', 'share',\n",
    "                                     'sticker', 'type', 'videos', 'photos'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe = dataframe[~dataframe['content'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe = dataframe[~dataframe[\"content\"].str.contains(\"http\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIA was used to calculate polarity for the group infographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sia = SIA()\n",
    "\n",
    "def compound_polarity(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "dataframe['polarity'] = dataframe['content'].apply(compound_polarity)\n",
    "\n",
    "#dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sender_name\n",
       "Ali Javed                  0.035944\n",
       "Ali Nawed                  0.064050\n",
       "Aushvin Vasanthakumaran    0.009103\n",
       "Diahanna Rose              0.059919\n",
       "Farhin Chowdhury           0.000000\n",
       "Hamza Ahsan                0.022621\n",
       "Harris Ismael              0.088019\n",
       "Janusshan Paramasivam     -0.015022\n",
       "Joey Bot                   0.121728\n",
       "Kishan Baskaran            0.067994\n",
       "Pavi Subenderan            0.035082\n",
       "Rogges Anandarajah         0.041745\n",
       "Saad Daas                  0.031565\n",
       "Sabil Ahmed                0.118182\n",
       "Thannoj Thavalingam        0.064037\n",
       "Vinoth Maruthalingam       0.063329\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = dataframe.groupby(\"sender_name\")\n",
    "grouped[\"polarity\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 4 cells run slow, especially the first two. For whatever reason, creating turning remove_contractions into a list from its original generator form takes inordinately long; like 30-45 minutes long despite a single object only taking miliseconds. Merits further investigation but it did deliver the data which is the expanded form of several contractions both formal and sland English.\n",
    "\n",
    "Update: On running out the generator, a massive delay occurs following the message \"if you cannot play defense you should not be in discussion for top 15 even\". Probably worth looking into.\n",
    "\n",
    "Update 2: The csv file of raw data and other processing showed nothing suspicious at that point. Weird..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who is going to kill you'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "cont.load_models()\n",
    "\n",
    "''.join(list(cont.expand_texts([\"Who's gonna kill u\"], precise = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_contractions = cont.expand_texts(list(dataframe['content']), precise = True)\n",
    "\n",
    "remove_contractions = list(remove_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_contractions = [str(text) for text in remove_contractions]\n",
    "\n",
    "dataframe[\"new_content\"] = remove_contractions\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single use cases for tokenizing and stemming words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yea', 'def', 'not', 'worth']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(dataframe[\"new_content\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nltk.stem.WordNetLemmatizer()\n",
    "a.lemmatize(\"mice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PorterStemmer doesn't work on curse words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fucker'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = nltk.PorterStemmer()\n",
    "b.stem(\"fucker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['But', 'today', 'like', 'come', 'on']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nltk.word_tokenize(dataframe['new_content'].iloc[1422])\n",
    "[a.lemmatize(word) for word in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords to remove, so more substantial language can be examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmstop combines generalized forms of all the single use cases above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmstop(text):\n",
    "    token = nltk.word_tokenize(text)\n",
    "    token = [word.lower() for word in token if word.isalpha()]\n",
    "    no_stop = [word for word in token if word not in stopwords]\n",
    "    lemm = [a.lemmatize(word) for word in no_stop]\n",
    "    return lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe['lemmatized'] = dataframe['new_content'].apply(lemmstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat counts up instances of words where the same letter is repeated two or more times. Those that occur 70 or more times will be included for further analysis. E.g. lmaoooo is the same as lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "repeated_letters = {}\n",
    "\n",
    "def repeat(word_list):\n",
    "    for word in word_list:\n",
    "        word = word.lower()\n",
    "        if re.findall(r'((\\w)\\2{2,})', word):\n",
    "            if word in repeated_letters:\n",
    "                repeated_letters[word] += 1\n",
    "            else:\n",
    "                repeated_letters[word] = 1\n",
    "\n",
    "dataframe['lemmatized'].apply(func = repeat)\n",
    "\n",
    "sorted_letter_repeats = sorted(repeated_letters.items(), key=op.itemgetter(1))\n",
    "\n",
    "sorted_letter_repeats[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_counter is to count up the amount of words. multi_letter_exceptions handles the instances of the same word being spelt with varying amounts of the same letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator as op\n",
    "word_counts = {}\n",
    "\n",
    "def word_counter(word_list):\n",
    "    for word in word_list:\n",
    "        multi_letter_exceptions = {'loool': 'lol', 'lmaooo': 'lmao', 'looool': \"lol\",\n",
    "                                  'lmaoooo': \"lmao\", 'yeahhh': 'yeah', 'ohhh': \"oh\",\n",
    "                                  'yeee': \"ye\"}\n",
    "        if word in multi_letter_exceptions:\n",
    "            word = multi_letter_exceptions[word]\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "dataframe['lemmatized'].apply(func = word_counter)\n",
    "\n",
    "sorted_word_counts = sorted(word_counts.items(), key=op.itemgetter(1))\n",
    "\n",
    "sorted_word_counts[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "More work to follow as time permits; topic extraction is slated next. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
